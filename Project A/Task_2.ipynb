{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMijEjJUPyKLMavWnhNWTH6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunhuanhuan920/ECE1512_2023F_ProjectRepo_Zhenhuan_Sun/blob/main/Project%20A/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Knowledge Distillation in MHIST Dataset**"
      ],
      "metadata": {
        "id": "Gvb3WAAceSEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "\n",
        "(a). MHIST dataset contains images of colorectal polyps. Both pre-trained ResNet50 and pre-trained MobileNetV2 do not possess the capability to classify colorectal polyps, as ImageNet does not contain any colorectal polyps examples. However, the first few layers of DNN, especially those trained on ImageNet, can usually capture general features in images like edges, textures, and shapes. These features are often generic and can be beneficial for many other visual classification tasks. By using the architecture of ResNet50 or MobileNetV2 and their trained weights, we can leverage their already-learned primitive features identifiers for our tasks. In addition, the final layer of DNNs is usually customized for the dataset they are trained on. For example, different dataset may contain different number of classes, and thus having different number of neurons in their last layers. By leveraging fine-tuning processes, even though the MHIST dataset have different classes than ImageNet, we can replace the output layer of ResNet50 and MobileNetV2 with custom layers tailored for MHIST, then freeze the per-trained layers and fine-tunes the parameters on those custom layers based on the MHIST dataset.\n",
        "\n",
        "(b). Residual block is a fundamental unit in Residual Network architecutres. It usually contains 2 convolutional layers with batch normalization layer attached after each, activation functions, and most importantly a residual path that connect the input of block to the output, creating skip connection.\n",
        "\n",
        "(c). Compared to ResNetV1 which has the following order within the residual block: Conv -> BN -> ReLU, the order within the residual block has changed to: BN -> ReLU -> Conv in ResNetV2. In addtion, in ResNetV1, ReLU activation is applied after the addition of the residual path. In ResNetV2, no activation is applied after addition.\n",
        "\n",
        "(d). MobileNetV1 used depthwise separable convolutions to reduce computational cost and build lightweight DNNs. MobileNetV2 build upon this by adding linear bottlenecks between the layers and connecting bottlenecks with residual path, which forms an inverted residual block.\n",
        "\n",
        "(e). This is due to the presence of residual connection in the residual block. For example, we can express the function in residual block as $f(x) + x$, where $f(x)$ is the function performed by the original layers and $+ x$ is the residual connection in the block. If we take the derivative of $f(x) + x$ with respect to $x$, we will get $\\frac{d}{dx} f(x) + 1$. The vanishing gradient problem is usually occured during backpropogation where gredients are repeatedly multiplied by some small number during the backpropogation along the $f(x)$ path, i.e., $\\frac{d}{dx} f(x)$. The $+ 1$ gives the backpropogated gradient a path to avoid the processing done in $f(x)$ and flow uninterruptly, which essentially mitigate the problem of vanishing gradient.\n",
        "\n",
        "(f). MobileNetV2 is considered as a lightweight model. It and its predecessor MobileNetV1 are designed for mobile devices with limited computation resources. Its architecture is tailored to provide a good balance between performance and computational efficiency, with fewer FLOPs (Floating Point Operations) and parameters compared to many other deep neural networks."
      ],
      "metadata": {
        "id": "x8v-34zReZ80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0qayhj2eMOz"
      },
      "outputs": [],
      "source": []
    }
  ]
}