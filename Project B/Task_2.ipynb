{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5iLcKY+6vQR+mJLFAMjLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunhuanhuan920/ECE1512_2023F_ProjectRepo_Zhenhuan_Sun/blob/main/Project%20B/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1"
      ],
      "metadata": {
        "id": "3u0_4HLlmWPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The papaper I chose is \"Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching\""
      ],
      "metadata": {
        "id": "EYRZnaNesrDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Conventional dataset distillation methods become less effective when the total number of synthetic samples becomes larger. By performing experiments, this paper has given us insights about why dataset distillation become less effective when image per class (IPC) increases, by which they proposed a method that allows trajectory matching-based dataset distillation methods remain effective in both low and high IPC settings. In addition, the proposed method has managed to achieve lossless dataset distillation, allowing synthetic dataset become as effective as the original large dataset in terms of training models.\n",
        "\n",
        "(b) Trajectory matching-based dataset distillation methods usually try to match all the update trajectories of models trained on both real and synthetic datasets across all training phases. However, the authors of this paper have found that matching late trajectories, i.e., learn \"hard\" patterns from original dataset, become more and more beneficial when IPC increases. While matching early trajectories, i.e., learn \"easy\" pattern from original dataset, is more beneficial with a low IPC and tends to be counterproductive with a high IPC. Building upon these insights, by calibrating the difficulty of the patterns to be learned, essentially avoiding learning patterns that are too easy or too hard, trajectory matchin-based method can remain effective in both low and high IPC settings through only matching the trajectories of a specified range, e.g., training phase.\n",
        "\n",
        "(c) The method initially acquires an expert training trajectory, denoted as $\\tau^* = \\{\\theta^*_t | 0 \\leq t \\leq n\\}$, which is essentially a sequence of model parameters captured at different time throughout the training of a network on the real dataset. The method then sets a lower bound $t = T^-$ and an upper bound $t = T^+$ on the range of the expert trajectory to indicate which model parameters can be sampled for matching, allowing the method to control the difficulty of the patterns to be learned. In addition, soft labels is used to allow more information to be contained in the synthetic dataset. To alleviate the insatability issue that the utilization of soft labels bring, a floating upper bound $T$ which is small initially and gradually approaches $T^+$ as distillation progresses, is set on the sample range to generate only \"easy\" pattern at the early distillation stage and gradually generate harder patterns. In each iteration, the start model parameter $\\theta^*_t$ and the target parameters $\\theta^*_{t+M}$ are sampled from the sample range for trajectory matching. The model parameter $\\hat{\\theta}_{t+N}$ obtained after training on synthetic dataset for $N$ steps is generated and the synthetic dataset is then updated by performing backpropagation from loss function, defined as\n",
        "$$L = \\frac{\\| \\hat{\\theta}_{t+N} - \\theta^*_{t+M} \\|^2_2}{\\| \\theta^*_{t} - \\theta^*_{t+M} \\|^2_2},$$\n",
        "to compute the graidents.\n",
        "\n",
        "(d)\n",
        "\n",
        "**Advantages**: By aligning the difficulty of the patterns to be learned with the size of the synthetic dataset, the trajectory matching- based method can produce more consistent results for both small and large synthetic datasets than traditional dataset distillation methods. In addition, this method has the potential to achieve lossless distillation, which allows the quality of the trained model to be maintained while using a significantly reduced dataset.\n",
        "\n",
        "**Disadvantages**: This method involves a complex process of setting different bounds, which increases the number of hyperparemeters needs to be tuned compared to the plain gradient matching algorithm we implemented in task 1. In addition, the adoption of soft labels in the synthetic dataset brings additional challenges, such as mislabeling and instability in distillation.\n",
        "\n",
        "The method's ability to control the difficulty of the patterns to be learned for synthetic dataset makes it very promising for distilling datasets, since it is capable of learning patterns that will be useful in both small and large IPC settings. In cases where the original datasets are extremely large and diverse, e.g., ImageNet, A small IPC setting might be ineffective as the limited number of synthetic samples may lack the capacity to encapsulate all the information contained in ImageNet. But, by increasing the IPC and adopting this method to focus on learning the \"hard\" patterns, it might be possible to distill more informative information from the large dataset into the synthetic dataset. However, its real-world efficacy would largely depend on how well the informative patterns can be learned from such extensive data, and would require empirical validation."
      ],
      "metadata": {
        "id": "p6j2B0zEmY-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2"
      ],
      "metadata": {
        "id": "avBdtJl8mX5v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc8Nf6a3mAXA"
      },
      "outputs": [],
      "source": []
    }
  ]
}