{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAu4No5XpQ3g0ASN1bM4OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunhuanhuan920/ECE1512_2023F_ProjectRepo_Zhenhuan_Sun/blob/main/Project%20B/Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Basic Concepts"
      ],
      "metadata": {
        "id": "5yYTJFemjxqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a)**. The purpose is to synthesize a small set of highly informative samples from a large dataset, by which deep neural network models can be trained in a more data efficient way. This condensed dataset aims to produce comparable performance to models trained on the full dataset, while requiring less memory and processing.\n",
        "\n",
        "**(b)**. Compared to conventional dataset distillation, dataset condensation with gradient matching can avoid performing a computation expensive nested loop optimization, which effectively make the optimization faster and more memory efficient. Thus, this method can scale up to larger deep enural network models.\n",
        "\n",
        "In addition, it can also generalize better to unseen data. This is due to its unique approach of matching the gradient of the neural network parameters trained on the original and synthetic data. This ensures that the essential learning signal, i.e., gradient flows, are retained, making the synthetic datset a more effective proxy for the original.\n",
        "\n",
        "**(c)**. The main novelty comes from the gradient matching. This method generate synthetic samples by minimizing the distance between gradients computed using the synthetic samples and gradients from the original dataset at each time step throughout the training process. In this way, the model trained using the synthetic dataset will try to mimic the update trajectory of the model parameters that would result from training on the original dataset.\n",
        "\n",
        "**(d)**. This method first defines an objective function that measures the distance between the gradients of model parameters obtained by training on the original dataset and those obtained by training on the synthetic dataset. The synthetic samples are generated by iteratively adjusting their values to minimize the objective function, effectively making the gradients induced by the synthetic dataset as close as possible to those induced by the original dataset. Due to the gradieent matching,the optimization is done by using gradient-based optimization techniques, where the synthetic data itself becomes the parameters that require updates and adjustment to minimize the gradient difference. Finally, the effectiveness of the synthetic dataset is validated by training a new model from scratch on this condensed dataset and evaluating its performance against a model trained on the original dataset.\n",
        "\n",
        "**(e)**. Continual Learning: In scenarios where models need to learn new tasks over time while maintaining performance on previous tasks, condensed datasets allows for more efficient memory use and more data-efficient continual learning, due to the utilization of a compact and informative set of data samples.\n",
        "\n",
        "Neural Architecture Search: When searching for the most effective neural network architectures, training different neural network models on large datasets can be computationally prohibitive. Condensed datasets allow for faster evaluation of different architectures, reducing the search time and computational cost, while still providing a reliable estimation of how well each architecture might perform if trained on the original full dataset."
      ],
      "metadata": {
        "id": "z_hoWsaakB_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset Distillation Learning"
      ],
      "metadata": {
        "id": "q8k0iLCPj3ON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxE_1SsNjigJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cross-architecture Generalization"
      ],
      "metadata": {
        "id": "eChSUB26j7g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Application"
      ],
      "metadata": {
        "id": "rVyyJ8Ivj_O-"
      }
    }
  ]
}